---
title: Data leakage inflates prediction performance in connectome-based machine learning
  models
authors:
- Matthew Rosenblatt
- Link Tejavibulya
- Rongtao Jiang
- Stephanie Noble
- Dustin Scheinost
date: '2024-02-01'
publishDate: '2024-07-05T12:03:55.170952Z'
publication_types:
- article-journal
publication: '*Nature Communications*'
doi: 10.1038/s41467-024-46150-w
abstract: Abstract Predictive modeling is a central technique in neuroimaging to identify
  brain-behavior relationships and test their generalizability to unseen data. However,
  data leakage undermines the validity of predictive models by breaching the separation
  between training and test data. Leakage is always an incorrect practice but still
  pervasive in machine learning. Understanding its effects on neuroimaging predictive
  models can inform how leakage affects existing literature. Here, we investigate
  the effects of five forms of leakage–involving feature selection, covariate correction,
  and dependence between subjects–on functional and structural connectome-based machine
  learning models across four datasets and three phenotypes. Leakage via feature selection
  and repeated subjects drastically inflates prediction performance, whereas other
  forms of leakage have minor effects. Furthermore, small datasets exacerbate the
  effects of leakage. Overall, our results illustrate the variable effects of leakage
  and underscore the importance of avoiding data leakage to improve the validity and
  reproducibility of predictive modeling.
links:
- name: URL
  url: https://www.nature.com/articles/s41467-024-46150-w
---
