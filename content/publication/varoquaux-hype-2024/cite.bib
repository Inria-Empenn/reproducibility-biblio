@misc{varoquaux_hype_2024,
 abstract = {With the growing attention and investment in recent AI approaches such as large language models, the narrative that the larger the AI system the more valuable, powerful and interesting it is is increasingly seen as common sense. But what is this assumption based on, and how are we measuring value, power, and performance? And what are the collateral consequences of this race to ever-increasing scale? Here, we scrutinize the current scaling trends and trade-offs across multiple axes and refute two common assumptions underlying the 'bigger-is-better' AI paradigm: 1) that improved performance is a product of increased scale, and 2) that all interesting problems addressed by AI require large-scale models. Rather, we argue that this approach is not only fragile scientifically, but comes with undesirable consequences. First, it is not sustainable, as its compute demands increase faster than model performance, leading to unreasonable economic requirements and a disproportionate environmental footprint. Second, it implies focusing on certain problems at the expense of others, leaving aside important applications, e.g. health, education, or the climate. Finally, it exacerbates a concentration of power, which centralizes decision-making in the hands of a few actors while threatening to disempower others in the context of shaping both AI research and its applications throughout society.},
 author = {Varoquaux, GaÃ«l and Luccioni, Alexandra Sasha and Whittaker, Meredith},
 doi = {10.48550/arXiv.2409.14160},
 file = {arXiv Fulltext PDF:/home/alpron/Zotero/storage/9WFZRACE/Varoquaux et al. - 2024 - Hype, Sustainability, and the Price of the Bigger-.pdf:application/pdf},
 keywords = {AI ethics, big, Computer Science - Computers and Society},
 month = {September},
 note = {arXiv:2409.14160 [cs]},
 publisher = {arXiv},
 title = {Hype, Sustainability, and the Price of the Bigger-is-Better Paradigm in AI},
 url = {http://arxiv.org/abs/2409.14160},
 urldate = {2024-10-01},
 year = {2024}
}
